{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOSPgOvkHLAfPTveABD3nl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patsoong/CS506FinalProject/blob/main/notebooks/Stack_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q4YGCp-Qfu3",
        "outputId": "926ba154-e5ad-400a-bdc3-17694762afb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stack ROC-AUC: 0.9806896551724138\n",
            "Stack Average Precision (PR-AUC): 0.7740458572435316\n",
            "Stack Binary Accuracy: 0.95\n",
            "Stack Top-1 accuracy: 0.7\n",
            "\n",
            "Predicted vs. True Champions by Season (Stacked Model):\n",
            "   season team_pred  pred_prob  team_true  correct\n",
            "0    2016  Warriors   0.982528  Cavaliers        0\n",
            "1    2017  Warriors   0.983654   Warriors        1\n",
            "2    2018  Warriors   0.981640   Warriors        1\n",
            "3    2019   Raptors   0.983918    Raptors        1\n",
            "4    2020     Bucks   0.500767     Lakers        0\n",
            "5    2021  Clippers   0.733325      Bucks        0\n",
            "6    2022  Warriors   0.980745   Warriors        1\n",
            "7    2023   Nuggets   0.977562    Nuggets        1\n",
            "8    2024   Celtics   0.983919    Celtics        1\n",
            "9    2025   Thunder   0.983811    Thunder        1\n",
            "\n",
            "Top-K Accuracy:\n",
            " Top-1: 0.7000\n",
            " Top-2: 0.9000\n",
            " Top-4: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "features_df = pd.read_csv(\"team_season_features_v2_clean-2.csv\")\n",
        "\n",
        "num_cols = features_df.select_dtypes(include=\"number\").columns.tolist() #remove features not to be used in training data\n",
        "for col in [\"champion\", \"season\"]:\n",
        "    if col in num_cols:\n",
        "        num_cols.remove(col)\n",
        "\n",
        "# add season-relative features (rank and z-score within each season)\n",
        "for col in num_cols:\n",
        "    features_df[f'{col}_season_rank'] = features_df.groupby('season')[col].rank(pct=True)\n",
        "    features_df[f'{col}_season_zscore'] = features_df.groupby('season')[col].transform(\n",
        "        lambda x: (x - x.mean()) / x.std() if x.std() > 0 else 0\n",
        "    )\n",
        "\n",
        "# update feature list\n",
        "num_cols_extended = features_df.select_dtypes(include=\"number\").columns.tolist()\n",
        "for col in [\"champion\", \"season\"]:\n",
        "    if col in num_cols_extended:\n",
        "        num_cols_extended.remove(col)\n",
        "\n",
        "X = features_df[num_cols_extended].copy()\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "y = features_df[\"champion\"].astype(int)\n",
        "\n",
        "#temporal split\n",
        "train_mask = features_df[\"season\"] <= 2015\n",
        "X_train, X_test = X[train_mask], X[~train_mask]\n",
        "y_train, y_test = y[train_mask], y[~train_mask]\n",
        "\n",
        "id_test = features_df.loc[~train_mask, [\"season\", \"team\", \"champion\"]].copy()\n",
        "\n",
        "log_reg = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
        "    (\"scale\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        multi_class='ovr',\n",
        "        class_weight='balanced',\n",
        "        max_iter=10000,\n",
        "        C=1,\n",
        "        random_state=42\n",
        "        ))\n",
        "])\n",
        "\n",
        "# svm_base = SVC(\n",
        "#     kernel=\"rbf\",\n",
        "#     C=1,\n",
        "#     gamma=0.01,\n",
        "#     class_weight=\"balanced\",\n",
        "#     probability=False,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# svm_cal = CalibratedClassifierCV(\n",
        "#     svm_base,\n",
        "#     method=\"sigmoid\",\n",
        "#     cv=3\n",
        "# )\n",
        "\n",
        "# svm = Pipeline([\n",
        "#     (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "#     (\"scale\", StandardScaler(with_mean=True, with_std=True)),\n",
        "#     (\"clf\", svm_cal),\n",
        "# ])\n",
        "\n",
        "xgboost = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scale\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", xgb.XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='auc',\n",
        "        use_label_encoder=False,\n",
        "        subsample=0.9,\n",
        "        scale_pos_weight=25.548387096774192,\n",
        "        n_estimators=100,\n",
        "        min_child_weight=3,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.01,\n",
        "        gamma=0,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "estimators = [\n",
        "    (\"logreg\", log_reg),\n",
        "    # (\"svm\", svm),\n",
        "    (\"xgboost\", xgboost)\n",
        "]\n",
        "\n",
        "# log_reg.fit(X_train, y_train)\n",
        "# log_reg_probs = log_reg.predict_proba(X_test)[:, 1]\n",
        "# svm.fit(X_train, y_train)\n",
        "# svm_probs = svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# alpha = 1\n",
        "# blend_probs = alpha * log_reg_probs + (1 - alpha) * svm_probs\n",
        "\n",
        "# print(\"Blend ROC-AUC:\", roc_auc_score(y_test, blend_probs))\n",
        "# print(\"Blend Average Precision (PR-AUC):\",\n",
        "#       average_precision_score(y_test, blend_probs))\n",
        "\n",
        "# blend_pred_labels = (blend_probs > 0.5).astype(int)\n",
        "# print(\"Blend Binary Accuracy:\", accuracy_score(y_test, blend_pred_labels))\n",
        "\n",
        "# id_test[\"blend_proba_win\"] = blend_probs\n",
        "\n",
        "# idx = id_test.groupby(\"season\")[\"blend_proba_win\"].idxmax()\n",
        "\n",
        "# blend_predicted_champs = (\n",
        "#     id_test.loc[idx, [\"season\", \"team\", \"blend_proba_win\"]]\n",
        "#           .rename(columns={\"team\": \"team_pred\",\n",
        "#                            \"blend_proba_win\": \"pred_prob\"})\n",
        "#           .reset_index(drop=True)\n",
        "# )\n",
        "\n",
        "# true_champs = (\n",
        "#     id_test.loc[id_test[\"champion\"] == 1, [\"season\", \"team\"]]\n",
        "#           .rename(columns={\"team\": \"team_true\"})\n",
        "#           .reset_index(drop=True)\n",
        "# )\n",
        "\n",
        "# blend_eval = blend_predicted_champs.merge(true_champs, on=\"season\", how=\"left\")\n",
        "# blend_eval[\"correct\"] = (blend_eval[\"team_pred\"] == blend_eval[\"team_true\"]).astype(int)\n",
        "\n",
        "# print(\"Blend Top-1 accuracy:\", blend_eval[\"correct\"].mean())\n",
        "# print(\"\\nPredicted vs. True Champions by Season (Blend):\")\n",
        "# print(blend_eval.sort_values(\"season\"))\n",
        "\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(\n",
        "        class_weight=\"balanced\",\n",
        "        max_iter=10000,\n",
        "        random_state=42\n",
        "    ),\n",
        "    stack_method=\"predict_proba\",\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "stack_probs = stack_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Stack ROC-AUC:\", roc_auc_score(y_test, stack_probs))\n",
        "print(\"Stack Average Precision (PR-AUC):\", average_precision_score(y_test, stack_probs))\n",
        "\n",
        "stack_pred_labels = (stack_probs > 0.5).astype(int)\n",
        "print(\"Stack Binary Accuracy:\", accuracy_score(y_test, stack_pred_labels))\n",
        "\n",
        "id_test[\"stack_proba_win\"] = stack_probs\n",
        "id_test[\"stack_ranking_score\"] = id_test[\"stack_proba_win\"]\n",
        "\n",
        "id_test[\"ranking_score\"] = id_test[\"stack_proba_win\"]\n",
        "\n",
        "id_test[\"rank\"] = (\n",
        "    id_test.groupby(\"season\")[\"ranking_score\"]\n",
        "           .rank(ascending=False, method=\"first\")\n",
        ")\n",
        "\n",
        "# pick team with highest prob in each season\n",
        "idx = id_test.groupby(\"season\")[\"stack_proba_win\"].idxmax()\n",
        "\n",
        "stack_predicted_champs = (\n",
        "    id_test.loc[idx, [\"season\", \"team\", \"stack_proba_win\"]]\n",
        "          .rename(columns={\"team\": \"team_pred\", \"stack_proba_win\": \"pred_prob\"})\n",
        "          .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "true_champs = (\n",
        "    id_test.loc[id_test[\"champion\"] == 1, [\"season\", \"team\"]]\n",
        "          .rename(columns={\"team\": \"team_true\"})\n",
        "          .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "stack_eval = stack_predicted_champs.merge(true_champs, on=\"season\", how=\"left\")\n",
        "stack_eval[\"correct\"] = (stack_eval[\"team_pred\"] == stack_eval[\"team_true\"]).astype(int)\n",
        "\n",
        "print(\"Stack Top-1 accuracy:\", stack_eval[\"correct\"].mean())\n",
        "print(\"\\nPredicted vs. True Champions by Season (Stacked Model):\")\n",
        "print(stack_eval.sort_values(\"season\"))\n",
        "\n",
        "# ranks of the true champions in their seasons\n",
        "true_champ_ranks = id_test.loc[id_test[\"champion\"] == 1, \"rank\"]\n",
        "\n",
        "# print top-k accuracies\n",
        "k_values = [1, 2, 4]   # or whatever kâ€™s you care about\n",
        "print(\"\\nTop-K Accuracy:\")\n",
        "for k in k_values:\n",
        "    accuracy = (true_champ_ranks <= k).mean()\n",
        "    print(f\" Top-{k}: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JIf8mJx-zqZd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}