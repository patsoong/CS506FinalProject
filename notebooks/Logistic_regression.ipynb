{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLEvqkGQFtUwGM4fUeF3YB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patsoong/CS506FinalProject/blob/main/notebooks/Logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hsvheP2nMNBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c47660-bc49-4693-b420-c2ad0d4023dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg ROC-AUC: 0.9817241379310344\n",
            "LogReg Average Precision (PR-AUC): 0.7043381180223285\n",
            "LogReg Binary accuracy: 0.9666666666666667\n",
            "LogReg Top-1 accuracy: 0.6\n",
            "\n",
            "Predicted vs. True Champions by Season:\n",
            "\n",
            " season team_pred team_true  pred_prob  correct\n",
            "   2016  Warriors Cavaliers   0.999938        0\n",
            "   2017  Warriors  Warriors   0.999984        1\n",
            "   2018  Warriors  Warriors   0.996921        1\n",
            "   2019   Raptors   Raptors   0.992933        1\n",
            "   2020     Bucks    Lakers   0.979083        0\n",
            "   2021  Clippers     Bucks   0.479522        0\n",
            "   2022  Warriors  Warriors   0.961972        1\n",
            "   2023   Celtics   Nuggets   0.949741        0\n",
            "   2024   Celtics   Celtics   0.999948        1\n",
            "   2025   Thunder   Thunder   0.999990        1\n",
            "\n",
            "Top-K Accuracy:\n",
            " Top-1: 0.6000\n",
            " Top-2: 0.9000\n",
            " Top-4: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "features_df = pd.read_csv(\"team_season_features_v2_clean-2.csv\")\n",
        "\n",
        "#remove features not to be used in training data\n",
        "num_cols = features_df.select_dtypes(include=\"number\").columns.tolist()\n",
        "for col in [\"champion\", \"season\"]:\n",
        "    if col in num_cols:\n",
        "        num_cols.remove(col)\n",
        "\n",
        "# add season-relative features (rank and z-score within each season)\n",
        "for col in num_cols:\n",
        "    features_df[f'{col}_season_rank'] = features_df.groupby('season')[col].rank(pct=True)\n",
        "    features_df[f'{col}_season_zscore'] = features_df.groupby('season')[col].transform(\n",
        "        lambda x: (x - x.mean()) / x.std() if x.std() > 0 else 0\n",
        "    )\n",
        "\n",
        "# update feature list\n",
        "num_cols_extended = features_df.select_dtypes(include=\"number\").columns.tolist()\n",
        "for col in [\"champion\", \"season\"]:\n",
        "    if col in num_cols_extended:\n",
        "        num_cols_extended.remove(col)\n",
        "\n",
        "X = features_df[num_cols_extended].copy()\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "y = features_df[\"champion\"].astype(int)\n",
        "\n",
        "#temporal split\n",
        "train_mask = features_df[\"season\"] <= 2015\n",
        "X_train, X_test = X[train_mask], X[~train_mask]\n",
        "y_train, y_test = y[train_mask], y[~train_mask]\n",
        "\n",
        "id_test = features_df.loc[~train_mask, [\"season\", \"team\", \"champion\"]].copy()\n",
        "\n",
        "# model creation\n",
        "model = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
        "    (\"scale\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        multi_class='ovr',\n",
        "        class_weight='balanced',\n",
        "        max_iter=10000,\n",
        "        C=0.05,\n",
        "        random_state=42\n",
        "        ))\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "probabilities = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Metrics\n",
        "print(\"LogReg ROC-AUC:\", roc_auc_score(y_test, probabilities))\n",
        "print(\"LogReg Average Precision (PR-AUC):\", average_precision_score(y_test, probabilities))\n",
        "\n",
        "pred_labels = (probabilities > 0.5).astype(int)\n",
        "print(\"LogReg Binary accuracy:\", accuracy_score(y_test, pred_labels))\n",
        "\n",
        "id_test[\"proba_win\"] = probabilities\n",
        "\n",
        "id_test[\"ranking_score\"] = id_test[\"proba_win\"]\n",
        "id_test[\"rank\"] = (\n",
        "    id_test.groupby(\"season\")[\"ranking_score\"]\n",
        "           .rank(ascending=False, method=\"first\")\n",
        ")\n",
        "\n",
        "idx = id_test.groupby(\"season\")[\"proba_win\"].idxmax()\n",
        "\n",
        "predicted_champs = (\n",
        "    id_test.loc[idx, [\"season\", \"team\", \"proba_win\"]]\n",
        "           .rename(columns={\"team\": \"team_pred\", \"proba_win\": \"pred_prob\"})\n",
        "           .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "true_champs = (\n",
        "    id_test.loc[id_test[\"champion\"] == 1, [\"season\", \"team\"]]\n",
        "           .rename(columns={\"team\": \"team_true\"})\n",
        "           .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "eval = predicted_champs.merge(true_champs, on=\"season\", suffixes=(\"_pred\", \"_true\"))\n",
        "eval[\"correct\"] = (eval[\"team_pred\"] == eval[\"team_true\"]).astype(int)\n",
        "print(\"LogReg Top-1 accuracy:\", eval[\"correct\"].mean())\n",
        "\n",
        "print(\"\\nPredicted vs. True Champions by Season:\\n\")\n",
        "print(eval[[\"season\", \"team_pred\", \"team_true\", \"pred_prob\", \"correct\"]]\n",
        "      .sort_values(\"season\")\n",
        "      .to_string(index=False))\n",
        "\n",
        "\n",
        "true_champ_ranks = id_test.loc[id_test[\"champion\"] == 1, \"rank\"]\n",
        "\n",
        "# print top-k accuracies\n",
        "k_values = [1, 2, 4]\n",
        "print(\"\\nTop-K Accuracy:\")\n",
        "for k in k_values:\n",
        "    accuracy = (true_champ_ranks <= k).mean()\n",
        "    print(f\" Top-{k}: {accuracy:.4f}\")"
      ]
    }
  ]
}