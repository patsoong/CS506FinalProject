{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796607cf",
   "metadata": {},
   "source": [
    "NN Using Softmax and Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0245e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 33. Best val top-1: 0.600\n",
      "Softmax NN (batched) Top-1 accuracy: 0.6\n",
      "\n",
      "Predicted vs True Champions (Softmax NN batched):\n",
      "2016 - Warriors (pred) | Cavaliers (true) | prob: 0.436 | correct: 0\n",
      "2017 - Warriors (pred) | Warriors (true) | prob: 0.562 | correct: 1\n",
      "2018 - Warriors (pred) | Warriors (true) | prob: 0.529 | correct: 1\n",
      "2019 - Raptors (pred) | Raptors (true) | prob: 0.295 | correct: 1\n",
      "2020 - Lakers (pred) | Lakers (true) | prob: 0.203 | correct: 1\n",
      "2021 - Clippers (pred) | Bucks (true) | prob: 0.206 | correct: 0\n",
      "2022 - Celtics (pred) | Warriors (true) | prob: 0.264 | correct: 0\n",
      "2023 - Celtics (pred) | Nuggets (true) | prob: 0.162 | correct: 0\n",
      "2024 - Celtics (pred) | Celtics (true) | prob: 0.449 | correct: 1\n",
      "2025 - Thunder (pred) | Thunder (true) | prob: 0.764 | correct: 1\n",
      "Test Average Precision (per season): 0.725\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Using Softmax for multiclass classification\n",
    "\n",
    "# Two Hidden layer MLP; ReLU for nonlinearity; Softmax at output layer\n",
    "# Each team produces single logit score\n",
    "class SeasonSoftmaxNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "        # Try to reduce overfitting, Regularization\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    # Scales for ReLu\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=0)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, X, lengths):\n",
    "        B, max_teams, F = X.shape\n",
    "\n",
    "        x = X.view(B * max_teams, F)\n",
    "\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        logits = self.out(x).view(B, max_teams)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class SeasonRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, dropout_p=0.15):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.kaiming_uniform_(self.out.weight, a=0)\n",
    "        nn.init.zeros_(self.out.bias)\n",
    "\n",
    "    def forward(self, X, lengths):\n",
    "        B, T, F = X.shape\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            X, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, _ = self.rnn(packed)\n",
    "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_out, batch_first=True, total_length=T\n",
    "        )\n",
    "\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        logits = self.out(rnn_out).squeeze(-1)\n",
    "\n",
    "        idx = torch.arange(T, device=lengths.device).unsqueeze(0)\n",
    "        mask = idx >= lengths.unsqueeze(1)\n",
    "        logits = logits.masked_fill(mask, -1e9)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DATA IMPORT\n",
    "\n",
    "df = pd.read_csv(\"team_season_features_v2_clean-2.csv\")\n",
    "\n",
    "num_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "for col in [\"champion\", \"season\"]:\n",
    "    if col in num_cols:\n",
    "        num_cols.remove(col)\n",
    "\n",
    "X_full = df[num_cols].replace([np.inf, -np.inf], np.nan).values\n",
    "\n",
    "\n",
    "# Temporal Training Split\n",
    "# ... -> 2010 : train\n",
    "# 2010 -> 2015 : validation\n",
    "# 2016... : test\n",
    "train_mask = df[\"season\"] <= 2010\n",
    "\n",
    "# Gets rid of NaNs for scaling\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_imp = imputer.fit_transform(X_full[train_mask])\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(X_train_imp)\n",
    "\n",
    "X_all_scaled = scaler.transform(imputer.transform(X_full))\n",
    "\n",
    "df_proc = df.copy()\n",
    "for i, col in enumerate(num_cols):\n",
    "    df_proc[col] = X_all_scaled[:, i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# BUILD DICT\n",
    "\n",
    "# Group by season\n",
    "def build_season_dict(df_proc, feature_cols):\n",
    "    season_data = {}\n",
    "    for season, g in df_proc.groupby(\"season\"):\n",
    "        X_season = g[feature_cols].values.astype(np.float32)\n",
    "        champ_idx_arr = np.where(g[\"champion\"].values == 1)[0]\n",
    "        if len(champ_idx_arr) != 1:\n",
    "            continue\n",
    "        champ_idx = int(champ_idx_arr[0])\n",
    "        teams = g[\"team\"].tolist()\n",
    "        season_data[season] = (X_season, champ_idx, teams)\n",
    "    return season_data\n",
    "\n",
    "season_data = build_season_dict(df_proc, num_cols)\n",
    "\n",
    "\n",
    "# Temporal Training Split\n",
    "# ... -> 2010 : train\n",
    "# 2010 -> 2015 : validation\n",
    "# 2016... : test\n",
    "train_seasons = {s: v for s, v in season_data.items() if s <= 2010}\n",
    "val_seasons   = {s: v for s, v in season_data.items() if 2010 < s <= 2015}\n",
    "test_seasons  = {s: v for s, v in season_data.items() if s > 2015}\n",
    "\n",
    "# padding; exactly 30 teams\n",
    "max_teams = max(X.shape[0] for (X, _, _) in season_data.values())\n",
    "feature_dim = len(num_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SeasonDataset\n",
    "\n",
    "# Padding features; team index\n",
    "class SeasonDataset(Dataset):\n",
    "    def __init__(self, season_dict, max_teams, feature_dim):\n",
    "        self.seasons = sorted(season_dict.keys())\n",
    "        self.X_list = []\n",
    "        self.lengths = []\n",
    "        self.champs = []\n",
    "        self.team_lists = []\n",
    "\n",
    "        for s in self.seasons:\n",
    "            X, champ_idx, teams = season_dict[s]\n",
    "            n_teams = X.shape[0]\n",
    "\n",
    "            X_pad = np.zeros((max_teams, feature_dim), dtype=np.float32)\n",
    "            X_pad[:n_teams, :] = X\n",
    "\n",
    "            self.X_list.append(X_pad)\n",
    "            self.lengths.append(n_teams)\n",
    "            self.champs.append(champ_idx)\n",
    "            self.team_lists.append(teams)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seasons)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"season\": self.seasons[idx],\n",
    "            \"X\": torch.from_numpy(self.X_list[idx]),\n",
    "            \"length\": self.lengths[idx],\n",
    "            \"champ_idx\": self.champs[idx],\n",
    "            \"teams\": self.team_lists[idx],\n",
    "        }\n",
    "\n",
    "# Create batches for train, val, and test\n",
    "def collate_fn(batch):\n",
    "    seasons = [b[\"season\"] for b in batch]\n",
    "    X = torch.stack([b[\"X\"] for b in batch], dim=0)\n",
    "    lengths = torch.tensor([b[\"length\"] for b in batch], dtype=torch.long)\n",
    "    champs = torch.tensor([b[\"champ_idx\"] for b in batch], dtype=torch.long)\n",
    "    teams = [b[\"teams\"] for b in batch]\n",
    "    return seasons, X, lengths, champs, teams\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_ds = SeasonDataset(train_seasons, max_teams, feature_dim)\n",
    "val_ds   = SeasonDataset(val_seasons,   max_teams, feature_dim)\n",
    "test_ds  = SeasonDataset(test_seasons,  max_teams, feature_dim)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                          collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "\n",
    "# Ensure prediction is confident\n",
    "def season_margin_loss(logits, champs, margin=0.5):\n",
    "    B, T = logits.shape\n",
    "\n",
    "    champ_logits = logits[torch.arange(B), champs]\n",
    "\n",
    "    mask = torch.ones_like(logits, dtype=torch.bool)\n",
    "    mask[torch.arange(B), champs] = False\n",
    "    non_champ_logits = logits.masked_fill(~mask, -1e9)\n",
    "\n",
    "    max_non_champ, _ = non_champ_logits.max(dim=1)\n",
    "\n",
    "    diff = champ_logits - max_non_champ\n",
    "    loss_margin = F.relu(margin - diff).mean()\n",
    "    return loss_margin\n",
    "\n",
    "\n",
    "# Choose model b/w RNN and MLP\n",
    "model = SeasonSoftmaxNN(feature_dim)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for seasons, X, lengths, champs, teams in loader:\n",
    "            X = X\n",
    "            lengths = lengths\n",
    "            champs = champs\n",
    "\n",
    "            logits = model(X, lengths)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == champs).sum().item()\n",
    "            total += champs.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "num_epochs = 200\n",
    "best_val_top1 = 0.0\n",
    "best_state = None\n",
    "\n",
    "patience = 20\n",
    "epochs_no_improve = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for seasons, X, lengths, champs, teams in train_loader:\n",
    "        logits = model(X, lengths)\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, champs, label_smoothing=0.1)\n",
    "        margin_loss = season_margin_loss(logits, champs, margin=0.5)\n",
    "\n",
    "        lambda_margin = .5\n",
    "        loss = ce_loss + lambda_margin* margin_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=3.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_train_loss = total_loss / max(1, n_batches)\n",
    "    val_top1 = evaluate(model, val_loader)\n",
    "\n",
    "    if val_top1 > best_val_top1 + 1e-4:\n",
    "        best_val_top1 = val_top1\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best val top-1: {best_val_top1:.3f}\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# EVAL\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_results = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seasons, X, lengths, champs, teams in test_loader:\n",
    "        logits = model(X, lengths)\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        pred_idx = probs.argmax()\n",
    "        true_idx = champs.item()\n",
    "        season = seasons[0]\n",
    "        team_list = teams[0]\n",
    "\n",
    "        pred_team = team_list[pred_idx]\n",
    "        true_team = team_list[true_idx]\n",
    "        is_correct = int(pred_idx == true_idx)\n",
    "\n",
    "        test_results.append(\n",
    "            (season, pred_team, true_team, probs[pred_idx], is_correct)\n",
    "        )\n",
    "        correct += is_correct\n",
    "        total += 1\n",
    "\n",
    "top1 = correct / total\n",
    "print(\"Softmax NN (batched) Top-1 accuracy:\", top1)\n",
    "\n",
    "print(\"\\nPredicted vs True Champions (Softmax NN batched):\")\n",
    "for season, pred_team, true_team, prob, ok in sorted(test_results):\n",
    "    print(season, \"-\", pred_team, \"(pred) |\", true_team, \"(true) |\",\n",
    "          \"prob:\", f\"{prob:.3f}\", \"| correct:\", ok)\n",
    "\n",
    "\n",
    "def evaluate_average_precision(model, loader):\n",
    "    model.eval()\n",
    "    ap_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seasons, X, lengths, champs, teams in loader:\n",
    "            logits = model(X, lengths)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            for b in range(probs.size(0)):\n",
    "                season_probs = probs[b].cpu().numpy()\n",
    "                champ_idx = champs[b].item()\n",
    "\n",
    "                sorted_indices = np.argsort(-season_probs)\n",
    "\n",
    "                rank = int(np.where(sorted_indices == champ_idx)[0][0]) + 1\n",
    "\n",
    "                ap = 1.0 / rank\n",
    "                ap_list.append(ap)\n",
    "\n",
    "    return float(np.mean(ap_list)) if ap_list else 0.0\n",
    "\n",
    "test_ap = evaluate_average_precision(model, test_loader)\n",
    "print(f\"Test Average Precision (per season): {test_ap:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aaa2ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   season champ_team  champ_prob  rank  top_team  top_prob\n",
      "0    2016  Cavaliers    0.064645     4  Warriors  0.436071\n",
      "1    2017   Warriors    0.562497     1  Warriors  0.562497\n",
      "2    2018   Warriors    0.529278     1  Warriors  0.529278\n",
      "3    2019    Raptors    0.294785     1   Raptors  0.294785\n",
      "4    2020     Lakers    0.202915     1    Lakers  0.202915\n",
      "5    2021      Bucks    0.067018     6  Clippers  0.205720\n",
      "6    2022   Warriors    0.203903     2   Celtics  0.263656\n",
      "7    2023    Nuggets    0.134823     3   Celtics  0.162370\n",
      "8    2024    Celtics    0.449129     1   Celtics  0.449129\n",
      "9    2025    Thunder    0.763685     1   Thunder  0.763685\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "season_stats = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seasons, X, lengths, champs, teams in test_loader:\n",
    "        logits = model(X, lengths)\n",
    "        probs = F.softmax(logits, dim=1)[0]\n",
    "\n",
    "        season = seasons[0]\n",
    "        champ_idx = champs.item()\n",
    "        champ_team = teams[0][champ_idx]\n",
    "        champ_prob = probs[champ_idx].item()\n",
    "\n",
    "        sorted_idx = torch.argsort(probs, descending=True)\n",
    "        rank = (sorted_idx == champ_idx).nonzero(as_tuple=True)[0].item() + 1\n",
    "\n",
    "        top_idx = sorted_idx[0].item()\n",
    "        top_team = teams[0][top_idx]\n",
    "        top_prob = probs[top_idx].item()\n",
    "\n",
    "        season_stats.append({\n",
    "            \"season\": season,\n",
    "            \"champ_team\": champ_team,\n",
    "            \"champ_prob\": champ_prob,\n",
    "            \"rank\": rank,\n",
    "            \"top_team\": top_team,\n",
    "            \"top_prob\": top_prob,\n",
    "        })\n",
    "\n",
    "season_df = pd.DataFrame(season_stats).sort_values(\"season\")\n",
    "print(season_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50252056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 2026 champion: Thunder (prob = 0.998)\n",
      "\n",
      "Top-5 2026 contenders:\n",
      "Thunder: 0.998\n",
      "Bucks: 0.002\n",
      "Heat: 0.000\n",
      "Lakers: 0.000\n",
      "Mavericks: 0.000\n",
      "Test Average Precision (per season): 0.725\n"
     ]
    }
   ],
   "source": [
    "df_2026 = pd.read_csv(\"2026_stats_december.csv\")\n",
    "\n",
    "X_2026_raw = df_2026[num_cols].replace([np.inf, -np.inf], np.nan).values\n",
    "X_2026_scaled = scaler.transform(imputer.transform(X_2026_raw)).astype(np.float32)\n",
    "\n",
    "n_teams_2026 = X_2026_scaled.shape[0]\n",
    "\n",
    "X_2026_pad = np.zeros((1, max_teams, feature_dim), dtype=np.float32)\n",
    "X_2026_pad[0, :n_teams_2026, :] = X_2026_scaled\n",
    "\n",
    "X_2026_tensor = torch.from_numpy(X_2026_pad)\n",
    "lengths_2026 = torch.tensor([n_teams_2026], dtype=torch.long)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_2026 = model(X_2026_tensor, lengths_2026)\n",
    "    probs_2026 = F.softmax(logits_2026, dim=1).cpu().numpy()[0]\n",
    "\n",
    "pred_idx_2026 = probs_2026.argmax()\n",
    "pred_team_2026 = df_2026.iloc[pred_idx_2026][\"team\"]\n",
    "pred_prob_2026 = probs_2026[pred_idx_2026]\n",
    "\n",
    "print(f\"Predicted 2026 champion: {pred_team_2026} (prob = {pred_prob_2026:.3f})\")\n",
    "\n",
    "topk = 5\n",
    "top_indices = probs_2026.argsort()[::-1][:topk]\n",
    "print(\"\\nTop-5 2026 contenders:\")\n",
    "for i in top_indices:\n",
    "    team = df_2026.iloc[i][\"team\"]\n",
    "    print(f\"{team}: {probs_2026[i]:.3f}\")\n",
    "\n",
    "\n",
    "def evaluate_average_precision(model, loader):\n",
    "    model.eval()\n",
    "    ap_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seasons, X, lengths, champs, teams in loader:\n",
    "            logits = model(X, lengths)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            for b in range(probs.size(0)):\n",
    "                season_probs = probs[b].cpu().numpy()\n",
    "                champ_idx = champs[b].item()\n",
    "\n",
    "                sorted_indices = np.argsort(-season_probs)\n",
    "\n",
    "                rank = int(np.where(sorted_indices == champ_idx)[0][0]) + 1\n",
    "\n",
    "                ap = 1.0 / rank\n",
    "                ap_list.append(ap)\n",
    "\n",
    "    return float(np.mean(ap_list)) if ap_list else 0.0\n",
    "\n",
    "test_ap = evaluate_average_precision(model, test_loader)\n",
    "print(f\"Test Average Precision (per season): {test_ap:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
